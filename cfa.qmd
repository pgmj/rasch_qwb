---
title: "Questionnaire on Well-Being (QWB)"
subtitle: "Psychometric analysis"
title-block-banner: "#009ca6"
title-block-banner-color: "#FFFFFF"
author: 
  name: Magnus Johansson
  affiliation: RISE Research Institutes of Sweden
  affiliation-url: https://www.ri.se/shic
  orcid: 0000-0003-1669-592X
date: last-modified
date-format: iso
always_allow_html: true
format: 
  html:
    toc: true
    toc-depth: 3
    toc-title: "Table of contents"
    embed-resources: true
    standalone: true
    page-layout: full
    mainfont: 'Lato'
    monofont: 'Roboto Mono'
    code-overflow: wrap
    code-fold: show
    code-tools: true
    code-link: true
    number-sections: true
    fig-dpi: 96
    layout-align: left
    linestretch: 1.6
    theme:
      - materia
      - custom.scss
    css: styles.css
    license: CC BY
  pdf:
    papersize: a4
    documentclass: report 
execute:
  echo: true
  warning: true
  message: false
  cache: true
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
bibliography: refs.bib
---

## Reproducing CFA

```{r}
library(readxl)
library(tidyverse)
library(lavaan)
library(patchwork)
library(knitr)
```

Questionnaire on Well-Being (QWB), 18 items, each item is scored on a scale of 0 to 4 [@hlynsson_evaluating_2024]. Data from the same paper. We'll use the data from the second study that was used in the CFA in the paper.

Code and data were retrieved from the paper's [OSF page](https://osf.io/gsc3r/). Really great to see these materials made available, it is such an important step towards improving the standards of science!

```{r}
# Read in study two data -------------------------------------------------------
dd <- read_excel("data/study_two.xlsx")

onefactor <- 'f1 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# Fit the model to the data
cfamodel <- sem(model = onefactor, data = dd, estimator = "WLSMV") 
```

This warning message is important! For WLSMV to work properly, one also needs to specify `ordered = TRUE`.

Let's see if we can reproduce the fit metrics reported in the paper (p.15), using the output from the misspecified function call above.

```{r}
cfamodel %>% summary(standardized=T, ci=F, fit.measures= TRUE, )
```

The "standard" column in the output looks like what has been reported in the paper (see quote below) regarding χ2, RMSEA, and CFI. Good to see that it is reproducible.

> A single-factor solution for the Confirmatory factor analysis for the Questionnaire on
> Well-Being. QWB resulted in a good fit for the data: χ2(135) = 603.03, p < 0.001,
> CFI = 0.988, SRMR = 0.053, RMSEA = 0.047 [90% CI: 0.043, 0.051]. Thus, our single-
> factor model for the QWB exhibits all of our predetermined criteria for a good model fit.

Let's run the CFA function call with `ordered = TRUE` added to make the WLSMV estimator, which was correctly described in the paper, work as intended.

```{r}
cfamodel2 <- sem(model = onefactor, data = dd, estimator = "WLSMV", ordered = TRUE)
cfamodel2 %>% summary(standardized=T, ci=F, fit.measures= TRUE, )
```

Before looking closer at the results and making comparisons to the published/reported metrics, we need to address the issue of reporting the correct, scaled model fit metrics.

The often used Hu & Bentler [-@hu_cutoff_1999] cutoff values (also used in the paper) are based on simulations of continuous data and ML estimation. As such, they are not appropriate for ordinal data analyzed with the WLSMV estimator [@mcneish_dynamic_2023;@savalei_computation_2018]. The R-package `dynamic` can produce [appropriate cutoff values](<https://rpubs.com/dmcneish/1025400>) for model fit indices. We'll get into that after reviewing the scaled fit metrics and modification indices.

### Scaled fit metrics

For WLSMV, [the .scaled metrics should be reported](https://rpubs.com/dmcneish/1025400).

```{r}
fit_metrics_scaled <- c("chisq.scaled", "df", "pvalue.scaled", 
                        "cfi.scaled", "tli.scaled", "rmsea.scaled", 
                        "rmsea.ci.lower.scaled","rmsea.ci.upper.scaled",
                        "srmr")

fitmeasures(cfamodel2, fit_metrics_scaled) %>% 
  rbind() %>% 
  as.data.frame() %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  rename(Chi2.scaled = chisq.scaled,
         p.scaled = pvalue.scaled,
         CFI.scaled = cfi.scaled,
         TLI.scaled = tli.scaled,
         RMSEA.scaled = rmsea.scaled,
         CI_low.scaled = rmsea.ci.lower.scaled,
         CI_high.scaled = rmsea.ci.upper.scaled,
         SRMR = srmr) %>% 
  knitr::kable()
```

Again, these were the metrics reported in the paper:

> A single-factor solution for the Confirmatory factor analysis for the Questionnaire on
> Well-Being. QWB resulted in a good fit for the data: χ2(135) = 603.03, p < 0.001,
> CFI = 0.988, SRMR = 0.053, RMSEA = 0.047 [90% CI: 0.043, 0.051]. Thus, our single-
> factor model for the QWB exhibits all of our predetermined criteria for a good model fit.

The differences from the model fit metrics output in the table above and those found in the paper are partially due to the missing `ordered = TRUE` option, but also from reporting the wrong metrics for the WLSMV estimator.

The correct model fit metrics indicate problems, no matter which cutoffs one would use, especially regarding RMSEA. Let us review the modification indices.

### Modification indices

We'll filter the list and only present those with mi/χ2 > 30.

```{r}
modificationIndices(cfamodel2,
                    standardized = T) %>% 
  as.data.frame(row.names = NULL) %>% 
  filter(mi > 30) %>% 
  arrange(desc(mi)) %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  knitr::kable()
```

Many very large mi/χ2 values due to residual correlations.

### Dynamic cutoff values

In order to establish useful cutoff values for the WLSMV estimator with ordinal data, we need to run simulations relevant to the current set of items and response data [@mcneish_dynamic_2023]. This has been implemented in the [development version](https://github.com/melissagwolf/dynamic?tab=readme-ov-file) of `dynamic`.

```{r}
library(dynamic) # devtools::install_github("melissagwolf/dynamic") for development version
```

```{r}
dyncut <- catOne(cfamodel2, reps = 500)
```

```{r}
dyncut
```

Explanations on Levels 0-3 from the `dynamic` [package vignette](https://rpubs.com/dmcneish/1025400):

> When there are 6 or more items, cfaOne will consider three levels of misspecification. As in catHB, the Level-0 row corresponds to the anticipated fit index values if the fitted model were the exact underlying population model. The Level-1 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between approximately 1/3 of item pairs. The Level-2 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between approximately 1/3 of item pairs. The Level-3 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between all item pairs.

As we can see, the observed/empirical fit metrics from the data does not come close to the Level-3 simulation based cutoff values.

## Summary comments

The 18 items do not fit a unidimensional model, due to issues with residual correlations and potential multidimensionality.

## Exploratory factor analysis

Let's look at the data using EFA. A lot of the code for this analysis was borrowed from <https://solomonkurz.netlify.app/blog/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/>

```{r}
f1 <- 'efa("efa")*f1 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# 2-factor model
f2 <- 'efa("efa")*f1 + 
       efa("efa")*f2 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# 3-factor
f3 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# 4-factor
f4 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# 5-factor
f5 <- '
efa("efa")*f1 +
efa("efa")*f2 +
efa("efa")*f3 +
efa("efa")*f4 + 
efa("efa")*f5 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

efa_f1 <- 
  cfa(model = f1,
      data = dd,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = TRUE)
efa_f2 <- 
  cfa(model = f2,
      data = dd,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = TRUE)
efa_f3 <- 
  cfa(model = f3,
      data = dd,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = TRUE)
efa_f4 <- 
  cfa(model = f4,
      data = dd,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = TRUE)
efa_f5 <- 
  cfa(model = f5,
      data = dd,
      rotation = "oblimin",
      estimator = "WLSMV",
      ordered = TRUE)

```

### Model fit table

```{r}
rbind(
  fitmeasures(efa_f1, fit_metrics_scaled),
  fitmeasures(efa_f2, fit_metrics_scaled),
  fitmeasures(efa_f3, fit_metrics_scaled),
  fitmeasures(efa_f4, fit_metrics_scaled),
  fitmeasures(efa_f5, fit_metrics_scaled)
  ) %>% 
  as.data.frame() %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  rename(Chi2.scaled = chisq.scaled,
         p.scaled = pvalue.scaled,
         CFI.scaled = cfi.scaled,
         TLI.scaled = tli.scaled,
         RMSEA.scaled = rmsea.scaled,
         CI_low.scaled = rmsea.ci.lower.scaled,
         CI_high.scaled = rmsea.ci.upper.scaled,
         SRMR = srmr) %>% 
  add_column(Model = paste0(1:5,"-factor"), .before = "Chi2.scaled") %>% 
  knitr::kable()
```

### Plot 4-factor EFA

```{r}
#| fig-height: 8
standardizedsolution(efa_f4) %>% 
  filter(op == "=~") %>% 
  mutate(item  = str_remove(rhs, "swb") %>% as.double(),
         factor = str_remove(lhs, "f")) %>% 
  # plot
  ggplot(aes(x = est.std, xmin = ci.lower, xmax = ci.upper, y = item)) +
  annotate(geom = "rect",
           xmin = -1, xmax = 1,
           ymin = -Inf, ymax = Inf,
           fill = "grey90") +
  annotate(geom = "rect",
           xmin = -0.7, xmax = 0.7,
           ymin = -Inf, ymax = Inf,
           fill = "grey93") +
  annotate(geom = "rect",
           xmin = -0.4, xmax = 0.4,
           ymin = -Inf, ymax = Inf,
           fill = "grey96") +
  geom_vline(xintercept = 0, color = "white") +
  geom_pointrange(aes(alpha = abs(est.std) < 0.4),
                  fatten = 10) +
  geom_text(aes(label = item, color = abs(est.std) < 0.4),
            size = 4) +
  scale_color_manual(values = c("white", "transparent")) +
  scale_alpha_manual(values = c(1, 1/3)) +
  scale_x_continuous(expression(lambda[standardized]), 
                     expand = c(0, 0), limits = c(-1, 1),
                     breaks = c(-1, -0.7, -0.4, 0, 0.4, 0.7, 1),
                     labels = c("-1", "-.7", "-.4", "0", ".4", ".7", "1")) +
  scale_y_continuous(breaks = 1:18, sec.axis = sec_axis(~ . * 1, breaks = 1:18)) +
  ggtitle("Factor loadings for the 4-factor model") +
  theme(legend.position = "none") +
  facet_wrap(~ factor, labeller = label_both) 
```

### EFA comments

As we saw in the CFA modification indices, I think most issues stem from residual correlations - some items are too similar and one in each correlated pair needs to be removed. 

Looking at the 4-factor solution, we have one factor with 1 item, two with 4 items, and one with 7 items.

  Let's review the 7 items with standardized loadings > 0.4 from the factor with most items in the 4-factor solution.

```{r}
#| warning: false
#| message: false
items <- standardizedsolution(efa_f4) %>% 
  filter(op == "=~",
         lhs == "f3",
         est.std > 0.4) %>% 
  pull(rhs)

itemlabels <- read_csv("data/itemlabels_swb.csv") %>% 
  mutate(itemnr = paste0("swb",1:18))

standardizedsolution(efa_f4) %>% 
  filter(op == "=~",
         lhs == "f3",
         est.std > 0.4) %>% 
  arrange(desc(est.std)) %>% 
  mutate_if(is.numeric, ~ round(.x, 3)) %>% 
  dplyr::select(!c(lhs,op,z,pvalue)) %>% 
  dplyr::rename(itemnr = rhs,
                loading = est.std) %>% 
  left_join(itemlabels, by = "itemnr") %>% 
  knitr::kable()
```

## Brief Rasch analysis

For fun, let's see how the 7 items from the EFA above work as a unidimensional scale using Rasch Measurement Theory.

```{r}
#| warning: false
#| message: false
library(RISEkbmRasch) # install first with `devtools::install_github("pgmj/RISEkbmRasch")`
df <- dd %>% 
  dplyr::select(all_of(items)) %>% 
  na.omit()
```

```{r}
#| column: margin
#| echo: false
RIlistItemsMargin(df, fontsize = 13)
```

::: panel-tabset
### Conditional item fit
```{r}
#| warning: false
#| message: false
simfit1 <- RIgetfit(df, iterations = 500, cpu = 8) 
RIitemfit(df, simfit1)
RIgetfitPlot(simfit1, df)
```
### CICC
```{r}
#| warning: false
#| message: false
ICCplot(as.data.frame(df), 
        itemnumber = 2, 
        method = "cut", 
        itemdescrip = c("item 8"))

### also suggested:
library(RASCHplot) # install first with `devtools::install_github("ERRTG/RASCHplot")`
CICCplot(PCM(df),
         which.item = 2,
         lower.groups = c(0,6,12,17,23),
         grid.items = FALSE)
```
### Item-restscore
```{r}
RIrestscore(df)
```
### PCA
```{r}
#| tbl-cap: "PCA of Rasch model residuals"
RIpcmPCA(df)
```
### Residual correlations
```{r}
simcor1 <- RIgetResidCor(df, iterations = 500, cpu = 8)
RIresidcorr(df, cutoff = simcor1$p99)

```
### 1st contrast loadings
```{r}
RIloadLoc(df)
```
### Response categories
```{r}
mirt(df, model=1, itemtype='Rasch', verbose = FALSE) %>% 
  plot(type="trace", as.table = TRUE, 
       theta_lim = c(-8,8))
```
### Targeting
```{r}
#| warning: false
#| message: false
#| fig-height: 5
# increase fig-height above as needed, if you have many items
RItargeting(df)
```
### Item hierarchy
```{r}
#| fig-height: 5
RIitemHierarchy(df)
```
### DIF score groups
```{r}
iarm::score_groups(as.data.frame(df)) %>% 
  as.data.frame(nm = "score_group") %>% 
  dplyr::count(score_group)

dif_plots <- df %>% 
  add_column(dif = iarm::score_groups(.)) %>% 
  mutate(dif = factor(dif, labels = c("Below median score","Above median score"))) %>% 
  split(.$dif) %>% # split the data using the DIF variable
  map(~ RItileplot(.x %>% dplyr::select(!dif)) + labs(title = .x$dif))
dif_plots[[1]] + dif_plots[[2]]
```
:::

### Rasch analysis 1 comments

Item 8 shows misfit and has a residual correlation with item 7. 

- swb7 - been able to object and to assert yourself when this is needed?
- swb8 - felt satisfied with your life in its present situation?

We'll remove item 8 and run the analysis again. Several other item pairs also have problematic residual correlations, but we'll start with removing one item and see how that affects the others.

We have no demographic information, which makes invariance/DIF difficult to evaluate. I tried splitting the data into score groups based on median score, but the high scoring group had too much missing data in lower response categories for analysis to be feasible.

```{r}
df$swb8 <- NULL
```

## Rasch analysis 2

```{r}
#| column: margin
#| echo: false
RIlistItemsMargin(df, fontsize = 13)
```

::: panel-tabset
### Conditional item fit
```{r}
simfit2 <- RIgetfit(df, iterations = 500, cpu = 8) 
RIitemfit(df, simfit2)
RIgetfitPlot(simfit2, df)
```
### CICC
```{r}
#| warning: false
#| message: false
ICCplot(as.data.frame(df), 
        itemnumber = c(2,4), 
        method = "cut", 
        itemdescrip = c("item 9","item 11"))

### also suggested:
library(RASCHplot) # install first with `devtools::install_github("ERRTG/RASCHplot")`
CICCplot(PCM(df),
         which.item = c(2,4),
         lower.groups = c(0,6,12,18),
         grid.items = TRUE)
```
### Item-restscore
```{r}
RIrestscore(df)
```
### PCA
```{r}
#| tbl-cap: "PCA of Rasch model residuals"
RIpcmPCA(df)
```
### Residual correlations
```{r}
simcor2 <- RIgetResidCor(df, iterations = 500, cpu = 8)
RIresidcorr(df, cutoff = simcor2$p99)

```
### 1st contrast loadings
```{r}
RIloadLoc(df)
```
### Targeting
```{r}
#| warning: false
#| message: false
#| fig-height: 5
# increase fig-height above as needed, if you have many items
RItargeting(df)
```
### Item hierarchy
```{r}
#| fig-height: 5
RIitemHierarchy(df)
```
### Person fit
```{r}
RIpfit(df)
```
:::

### Rasch analysis 2 comments

Item 9 is slightly high in item fit and has residual correlations with both item 7 and 10. We'll remove it next

- 9: felt that your life is meaningful?
- 10: slept well and got the right amount of sleep?
- 7: been able to object and to assert yourself when this is needed

These are, from my perspective, a bit unexpected residual correlation pairs. While sleep certainly is important for quality of life, I would not expect a strong residual correlation with "life is meaningful". And item 9 and 7 seem even more unexpected. But these are just my reflections.


```{r}
df$swb9 <- NULL
```

## Rasch analysis 3

```{r}
#| column: margin
#| echo: false
RIlistItemsMargin(df, fontsize = 13)
```

::: panel-tabset
### Conditional item fit
```{r}
simfit3 <- RIgetfit(df, iterations = 1000, cpu = 8) 
RIitemfit(df, simfit3)
RIgetfitPlot(simfit3, df)
```
### CICC
```{r}
#| warning: false
#| message: false
ICCplot(as.data.frame(df), 
        itemnumber = c(2), 
        method = "cut", 
        itemdescrip = c("item 10"))

CICCplot(PCM(df),
         which.item = c(2),
         lower.groups = c(0,5,10,15),
         grid.items = FALSE)
```
### Item-restscore
```{r}
RIrestscore(df)
```
### PCA
```{r}
#| tbl-cap: "PCA of Rasch model residuals"
RIpcmPCA(df)
```
### Residual correlations
```{r}
simcor3 <- RIgetResidCor(df, iterations = 500, cpu = 8)
RIresidcorr(df, cutoff = simcor3$p99)

```
### 1st contrast loadings
```{r}
RIloadLoc(df)
```
### Targeting
```{r}
#| warning: false
#| message: false
#| fig-height: 5
# increase fig-height above as needed, if you have many items
RItargeting(df)
```
### Item hierarchy
```{r}
#| fig-height: 5
RIitemHierarchy(df)
```
### Reliability
```{r}
#| warning: false
#| message: false
RItif(df, cutoff = 2, samplePSI = TRUE)
SepRel(PCM(df) %>% person.parameter())
```
### Person fit
```{r}
RIpfit(df)
```
### Item parameters
```{r}
RIitemparams(df)
```
### Sum score->Interval score
```{r}
RIscoreSE(df, output = "figure")
RIscoreSE(df)
```
:::

### Rasch analysis 3 comments

Item 10 now shows slightly high item fit, while item-restscore looks ok for all items.

Item 17 has two residual correlations slightly above the cutoff, with items 11 and 15.

- 17: had the power to recover one’s strength if something has been stressful or difficult?
- 11: have been able to be focused and concentrated on today’s tasks?
- 15: been able to make decisions and carry them out?

Model fit could probably be improved further by removing item 10 or 17, but we'll leave it for now.

Overall, these are five items with decent psychometric properties. The TIF curve could be better and targeting shows a minor ceiling effect, which all well-being questionnaires seem to have to some degree.

## CFA with 5 items

Based on the previous Rasch analysis.

```{r}
fiveitems <- 'f1 =~ swb7 + swb10 + swb11 + swb15 + swb17'
m5 <- cfa(model = fiveitems,
    data = df,
    estimator = "WLSMV", ordered = TRUE)
summary(m5, standardized = TRUE)
```

All factor loadings are around 0.80.

### Model fit

We need the simulation based cutoff values to make sense of model fit metrics.

```{r}
dyncut2 <- catOne(m5, reps = 250)
```

```{r}
dyncut2
```

These are a bit better than level-2 values, which seems acceptable.

### Modification indices

```{r}
modificationIndices(m5,
                    standardized = T) %>% 
  as.data.frame(row.names = NULL) %>% 
  filter(mi > 3) %>% 
  arrange(desc(mi)) %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  knitr::kable()
```

Similarly to the Rasch analysis, there are some residual correlations. We can see item 17 involved in three of the six correlated item pairs.

### CTT reliability

Classical test theory assumes reliability to be a constant value across the latent continuum and across all participants.

```{r}
omega(df, nfactors = 1, poly = TRUE)
alpha(df)
```

## Software used
```{r}
sessionInfo()
```

## References

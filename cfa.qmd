---
title: "Questionnaire on Well-Being (QWB)"
subtitle: "CFA"
title-block-banner: "#009ca6"
title-block-banner-color: "#FFFFFF"
author: 
  name: Magnus Johansson
  affiliation: RISE Research Institutes of Sweden
  affiliation-url: https://www.ri.se/shic
  orcid: 0000-0003-1669-592X
date: last-modified
date-format: iso
always_allow_html: true
format: 
  html:
    toc: true
    toc-depth: 3
    toc-title: "Table of contents"
    embed-resources: true
    standalone: true
    page-layout: full
    mainfont: 'Lato'
    monofont: 'Roboto Mono'
    code-overflow: wrap
    code-fold: false
    code-tools: true
    code-link: true
    number-sections: true
    fig-dpi: 96
    layout-align: left
    linestretch: 1.6
    theme:
      - materia
      - custom.scss
    css: styles.css
    license: CC BY
  pdf:
    papersize: a4
    documentclass: report 
execute:
  echo: true
  warning: true
  message: false
  cache: true
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
bibliography: refs.bib
---

## Reproducing CFA

```{r}
library(readxl)
library(tidyverse)
library(lavaan)
library(patchwork)
```

Questionnaire on Well-Being (QWB), 18 items, each item is scored on a scale of 0 to 4 [@hlynsson_evaluating_2024]. Data from the same paper. We'll use the data from the second study that was used in the CFA in the paper.

Code and data were retrieved from the paper's [OSF page](https://osf.io/gsc3r/). Really great to see these materials made available, it is such an important step towards improving the standards of science!

```{r}
# Read in study two data -------------------------------------------------------
dd <- read_excel("data/study_two.xlsx")

onefactor <- 'f1 =~ swb1 + swb2 + swb3 + swb4 + swb5 + swb6 + swb7 + swb8 +
                    swb9 + swb10 + swb11 + swb12 + swb13 + swb14 + swb15 + 
                    swb16 + swb17 + swb18'

# Fit the model to the data
cfamodel <- sem(model = onefactor, data = dd, estimator = "WLSMV") 
```

This warning message is important! For WLSMV to work properly, one also needs to specify `ordered = TRUE`.

Let's see if we can reproduce the fit metrics reported in the paper (p.15), using the output from the misspecified function call above.

```{r}
cfamodel %>% summary(standardized=T, ci=F, fit.measures= TRUE, )
```

The "standard" column in the output looks like what has been reported in the paper (see quote below) regarding χ2, RMSEA, and CFI. Good to see that it is reproducible.

> A single-factor solution for the Confirmatory factor analysis for the Questionnaire on
> Well-Being. QWB resulted in a good fit for the data: χ2(135) = 603.03, p < 0.001,
> CFI = 0.988, SRMR = 0.053, RMSEA = 0.047 [90% CI: 0.043, 0.051]. Thus, our single-
> factor model for the QWB exhibits all of our predetermined criteria for a good model fit.

Let's run the CFA function call with `ordered = TRUE` added to make the WLSMV estimator, which was correctly described in the paper, work as intended.

```{r}
cfamodel2 <- sem(model = onefactor, data = dd, estimator = "WLSMV", ordered = TRUE)
cfamodel2 %>% summary(standardized=T, ci=F, fit.measures= TRUE, )
```

Before looking closer at the results and making comparisons to the published/reported metrics, we need to address the issue of reporting the correct model fit metrics. For WLSMV, the [.scaled metrics should be used](https://rpubs.com/dmcneish/1025400). 

Also, the Hu & Bentler [-@hu_cutoff_1999] cutoff values references in the paper are not appropriate for ordinal data analyzed with the WLSMV estimator [@mcneish_dynamic_2023;@savalei_computation_2018]. The R-package `dynamic` can produce [appropriate cutoff values](<https://rpubs.com/dmcneish/1025400>) for model fit indices. We'll get into that after reviewing the scaled fit metrics.

### Scaled fit metrics

Another important issue is reporting the correct model fit metrics. For WLSMV, [the .scaled metrics should be used](https://rpubs.com/dmcneish/1025400).

```{r}
fit_metrics_scaled <- c("chisq.scaled", "df", "pvalue.scaled", 
                        "cfi.scaled", "tli.scaled", "rmsea.scaled", 
                        "rmsea.ci.lower.scaled","rmsea.ci.upper.scaled",
                        "srmr")

fitmeasures(cfamodel2, fit_metrics_scaled) %>% 
  rbind() %>% 
  as.data.frame() %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  rename(Chi2.scaled = chisq.scaled,
         p.scaled = pvalue.scaled,
         CFI.scaled = cfi.scaled,
         TLI.scaled = tli.scaled,
         RMSEA.scaled = rmsea.scaled,
         CI_low.scaled = rmsea.ci.lower.scaled,
         CI_high.scaled = rmsea.ci.upper.scaled,
         SRMR = srmr) %>% 
  knitr::kable()
```

Again, these were the metrics reported in the paper:

> A single-factor solution for the Confirmatory factor analysis for the Questionnaire on
> Well-Being. QWB resulted in a good fit for the data: χ2(135) = 603.03, p < 0.001,
> CFI = 0.988, SRMR = 0.053, RMSEA = 0.047 [90% CI: 0.043, 0.051]. Thus, our single-
> factor model for the QWB exhibits all of our predetermined criteria for a good model fit.

The differences from the model fit metrics output in the table above and found in the paper are partially due to the missing `ordered = TRUE` option, but also from reporting the wrong metrics for the WLSMV estimator.

The correct model fit metrics indicate problems, no matter which cutoffs one would use, especially regarding RMSEA. Let us review the modification indices.

### Modification indices

We'll filter the list and only present those with mi/χ2 > 30.

```{r}
modificationIndices(cfamodel2,
                    standardized = T) %>% 
  as.data.frame(row.names = NULL) %>% 
  filter(mi > 30) %>% 
  arrange(desc(mi)) %>% 
  mutate(across(where(is.numeric),~ round(.x, 3))) %>%
  knitr::kable()
```

Many very large mi/χ2 values due to residual correlations.

### Dynamic cutoff values

As mentioned earlier, the often used Hu & Bentler [-@hu_cutoff_1999] cutoff values are based on simulations of continuous data and ML estimation. In order to establish useful cutoff values for the WLSMV estimator with ordinal data, we need to run simulations relevant to the current set of items and response data [@mcneish_dynamic_2023]. This has been implemented in the [development version](https://github.com/melissagwolf/dynamic?tab=readme-ov-file) of `dynamic`.

```{r}
library(dynamic) # devtools::install_github("melissagwolf/dynamic") for development version
```

```{r}
dyncut <- catOne(cfamodel2, reps = 500)
```

```{r}
dyncut
```

Explanations on Levels 0-3 from the `dynamic` [package vignette](https://rpubs.com/dmcneish/1025400):

> When there are 6 or more items, cfaOne will consider three levels of misspecification. As in catHB, the Level-0 row corresponds to the anticipated fit index values if the fitted model were the exact underlying population model. The Level-1 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between approximately 1/3 of item pairs. The Level-2 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between approximately 1/3 of item pairs. The Level-3 row corresponds to the anticipated fit index values if the fitted model omitted 0.30 residual correlations between all item pairs.

As we can see, the observed/empirical fit metrics from the data does not come close to even the Level-3 simulation based cutoff values.

## Summary comments

The 18 items do not fit a unidimensional model, due to issues with residual correlations and potential multidimensionality.

## Software used
```{r}
sessionInfo()
```

## References
